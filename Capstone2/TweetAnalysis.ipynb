{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages \n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re, string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sr No                                              tweet  label\n",
      "0      1  Hysteria surrounding #coronavirus NZ daycare r...      3\n",
      "1      2  Thank you @TheOnion for dragging all of us und...      1\n",
      "2      3  #avetmissdone is catching on faster than the #...      1\n",
      "3      4  They just said #Tonysnell was back from the fl...      2\n",
      "4      5  Forget locking them up on an island to die slo...      2\n",
      "             Sr No         label\n",
      "count  10000.00000  10000.000000\n",
      "mean    5000.50000      1.879200\n",
      "std     2886.89568      0.784518\n",
      "min        1.00000      1.000000\n",
      "25%     2500.75000      1.000000\n",
      "50%     5000.50000      2.000000\n",
      "75%     7500.25000      3.000000\n",
      "max    10000.00000      3.000000\n"
     ]
    }
   ],
   "source": [
    "#import tweet data\n",
    "data = pd.read_csv(\"tweets_tagged.csv\",delimiter=',',encoding='latin-1')\n",
    "print(data.head())\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you @TheOnion for dragging all of us under the artful guise of satire. V helpful #coronavirus advice ?? https://t.co/Oly95Sul5f\n",
      "Thank you @TheOnion for dragging all of us under the artful guise of satire. V helpful #coronavirus advice ?? \n"
     ]
    }
   ],
   "source": [
    "#remove urls \n",
    "def remove_URL(sample):\n",
    "    \"\"\"Remove URLs from a string\"\"\"\n",
    "    return re.sub(r\"http\\S+\", \"\", sample)\n",
    "print(data['tweet'][1])\n",
    "data['tweet']=[remove_URL(tweet) for tweet in data['tweet']]\n",
    "print(data['tweet'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Hysteria surrounding #coronavirus NZ daycare r...\n",
      "1    Thank you @TheOnion for dragging all of us und...\n",
      "2    #avetmissdone is catching on faster than the #...\n",
      "5    Please keep posted!?????? #CoronaVirus #Gensan...\n",
      "6     What did Richard Jefferson say? ?? #coronavirus \n",
      "Name: tweet, dtype: object\n",
      "0.5958730158730159\n"
     ]
    }
   ],
   "source": [
    "#seperate dataframe to positive, negative, and both tweets\n",
    "pos_tweets =data['tweet'][data['label']==1]\n",
    "neg_tweets = data['tweet'][data['label']==3]\n",
    "pos_neg_tweets = data['tweet'][data['label']!=2]\n",
    "\n",
    "#examine the data\n",
    "print(pos_neg_tweets.head())\n",
    "print(len(pos_tweets)/len(pos_neg_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to lower case  \n",
    "pos_neg_tweets = [tweet.lower() for tweet in pos_neg_tweets]\n",
    "pos_tweets = [tweet.lower() for tweet in pos_tweets]\n",
    "neg_tweets = [tweet.lower() for tweet in neg_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thank you @theonion for dragging all of us under the artful guise of satire. v helpful #coronavirus advice ?? ']\n",
      "['hysteria surrounding #coronavirus nz daycare requesting all children who have visited a country with any confirmed cases be excluded for 2 weeks. this includes australia. so, despite us only visiting adelaide where there are no confirmed cases, we are in this category ??']\n"
     ]
    }
   ],
   "source": [
    "print(pos_tweets[:1])\n",
    "print(neg_tweets[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#avetmissdone', 'is', 'catching', 'on', 'faster', 'than', 'the', '#coronavirus', '...', 'but', 'no', 'deaths', 'yet', '!', '?', '?']\n",
      "['please', 'keep', 'posted', '!', '?', '?', '?', '#coronavirus', '#gensan', '#stelizabethhospitalinc', '#sehi']\n",
      "['thank', 'you', '@theonion', 'for', 'dragging', 'all', 'of', 'us', 'under', 'the', 'artful', 'guise', 'of', 'satire', '.', 'v', 'helpful', '#coronavirus', 'advice', '?', '?']\n"
     ]
    }
   ],
   "source": [
    "#use Tweet tokenizer (preserves hashtag and @)\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "pos_tokens = [tknzr.tokenize(tweet) for tweet in pos_tweets]\n",
    "neg_tokens = [tknzr.tokenize(tweet) for tweet in neg_tweets]\n",
    "tweet_tokens = [tknzr.tokenize(tweet) for tweet in pos_neg_tweets]\n",
    "print(pos_tokens[1])\n",
    "print(neg_tokens[1])\n",
    "print(tweet_tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('thank', 'NN'), ('you', 'PRP'), ('@theonion', 'VBP'), ('for', 'IN'), ('dragging', 'VBG'), ('all', 'DT'), ('of', 'IN'), ('us', 'PRP'), ('under', 'IN'), ('the', 'DT'), ('artful', 'JJ'), ('guise', 'NN'), ('of', 'IN'), ('satire', 'NN'), ('.', '.'), ('v', 'NN'), ('helpful', 'JJ'), ('#coronavirus', 'NN'), ('advice', 'NN'), ('?', '.'), ('?', '.')]\n"
     ]
    }
   ],
   "source": [
    "#normalization \n",
    "#tagging position of word in sentence\n",
    "#show example for first tweet\n",
    "print(pos_tag(tweet_tokens[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove noise and lemmatize (copy and pasted from blog)\n",
    "\n",
    "    #I dont know how to remove the following : ''...' '..' '``'\n",
    "    #I found this command to delete all punctuation: s.translate(str.maketrans('', '', string.punctuation)). how would I apply it?\n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        #token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token) #this would delete @handles?\n",
    "        token=re.sub(\"([^A-Za-z#])\",\"\", token) #this deletes all numbers and punctuations, though it also transforms #ncov19 to #ncov\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "        #use if condition to drop single character strings\n",
    "        if len(token) > 1 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define stopwords to remove\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply remove_noise function\n",
    "pos_tokens_cleaned = []\n",
    "neg_tokens_cleaned = []\n",
    "tweet_tokens_cleaned = []\n",
    "\n",
    "for tokens in pos_tokens:\n",
    "    pos_tokens_cleaned.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "for tokens in neg_tokens:\n",
    "    neg_tokens_cleaned.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "for tokens in tweet_tokens:\n",
    "    tweet_tokens_cleaned.append(remove_noise(tokens, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pos_tokens_cleaned[0])\n",
    "print(neg_tokens_cleaned[0])\n",
    "print(tweet_tokens_cleaned[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequencies with get_all_words\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "all_tokens = get_all_words(tweet_tokens_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word frequencies for all words - this does not allow to plot a histogram\n",
    "freq_dist = FreqDist(all_tokens)\n",
    "print(freq_dist.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count words with counter function to later drop most frequent words and plot a histogram\n",
    "\n",
    "word_counter={}\n",
    "for tweet in tweet_tokens_cleaned:\n",
    "    for word in tweet:\n",
    "        if word != '':\n",
    "            if word in word_counter:\n",
    "                word_counter[word] +=1\n",
    "            else:\n",
    "                word_counter[word] = 1\n",
    "\n",
    "popular_words = sorted(word_counter, key = word_counter.get, reverse = True)\n",
    "top_50=popular_words[:50]\n",
    "top_10 = popular_words[:10]\n",
    "#print top 50 words\n",
    "print(top_50)  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot most distribution of 50 most common words\n",
    "#get values of word_counter for most common 50 words\n",
    "freq = []\n",
    "words = []\n",
    "sorted_freq=dict(sorted(word_counter.items(), key=lambda item: item[1], reverse=True))\n",
    "for key, value in sorted_freq.items():\n",
    "    if key in top_50:\n",
    "        freq.append(value)\n",
    "        words.append(key)\n",
    "#make bar plot of most frequent words\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plt.bar(height=freq, x=words, width=0.8)\n",
    "plt.xticks(rotation=90)\n",
    "plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make list of pos and negative words from list of tweets to be able to delete most frequent words and count words looping over list\n",
    "pos_words = []\n",
    "neg_words = []\n",
    "\n",
    "for tweet in pos_tokens_cleaned:\n",
    "    for word in tweet:\n",
    "        pos_words.append(word)\n",
    "        \n",
    "for tweet in neg_tokens_cleaned:\n",
    "    for word in tweet:\n",
    "        neg_words.append(word)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count and plot top words in positive and negative tweets that are not in top50\n",
    "count_pos={}\n",
    "for word in pos_words:\n",
    "    if word not in top_50:\n",
    "        if word in count_pos:\n",
    "            count_pos[word] +=1\n",
    "        else:\n",
    "            count_pos[word] = 1\n",
    "\n",
    "popular_pos_words = sorted(count_pos, key = count_pos.get, reverse = True)\n",
    "top_20_pos_words=popular_pos_words[:20]\n",
    "count_neg={}\n",
    "for word in neg_words:\n",
    "    if word not in top_50:\n",
    "        if word in count_neg:\n",
    "            count_neg[word] +=1\n",
    "        else:\n",
    "            count_neg[word] = 1\n",
    "\n",
    "popular_neg_words = sorted(count_neg, key = count_neg.get, reverse = True)\n",
    "top_20_neg_words=popular_neg_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get frequencies from dictionaries in a sorted way as tuples for plotting top20\n",
    "#positive\n",
    "freq_pos = []\n",
    "words_pos = []\n",
    "sorted_freq_pos= [(k, v) for k, v in count_pos.items()]\n",
    "sorted_freq_pos=sorted(sorted_freq_pos, key=lambda tup: tup[1], reverse=True)\n",
    "words_pos = [a for (a,_) in sorted_freq_pos]\n",
    "freq_pos = [b for (_,b) in sorted_freq_pos]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#negative\n",
    "freq_neg = []\n",
    "words_neg = []\n",
    "sorted_freq_neg= [(k, v) for k, v in count_neg.items()]\n",
    "sorted_freq_neg=sorted(sorted_freq_neg, key=lambda tup: tup[1], reverse=True)\n",
    "words_neg = [a for (a,_) in sorted_freq_neg]\n",
    "freq_neg = [b for (_,b) in sorted_freq_neg]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make bar plot of frequencies - I don't know how to improve readability of axis labels\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "#This will create first plot\n",
    "neg = plt.bar(words_neg[:20], freq_neg[:20])\n",
    "plt.title('Negative tweets')\n",
    "plt.ylabel('Word frequency')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.tick_params(axis='both', which='major', labelsize=13)\n",
    "#The below code will create the second plot.\n",
    "plt.subplot(1,2,2)\n",
    "#This will create the second plot\n",
    "pos =plt.bar(words_pos[:20], freq_pos[:20])\n",
    "plt.title('Positive tweets')\n",
    "plt.xticks( rotation='vertical')\n",
    "plt.tick_params(axis='both', which='major', labelsize=13)\n",
    "plt.show()     \n",
    "plt.suptitle('Most common words in positive and negative tweets')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing data for modelling\n",
    "#make dict out of lists of tokens with keys = tokens and values=true\n",
    "\n",
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(pos_tokens_cleaned)\n",
    "negative_tokens_for_model = get_tweets_for_model(neg_tokens_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test and training set split for Naive Bayes model\n",
    "#joining positive and negative tweets with additional tag, and shuffle into 1 set\n",
    "import random\n",
    "\n",
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                     for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                     for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "random.shuffle(dataset)\n",
    "\n",
    "#we have 6,300 tweets after dropping the neutral ones\n",
    "train_size=int(len(dataset)*0.7) # set training dataset size to 0.7 of all data\n",
    "print(train_size)\n",
    "train_data = dataset[:train_size]\n",
    "test_data = dataset[train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building model\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
