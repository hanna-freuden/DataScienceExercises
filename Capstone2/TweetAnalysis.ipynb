{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages \n",
    "import nltk\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sr No                                              tweet  label\n",
      "0      1  Hysteria surrounding #coronavirus NZ daycare r...      3\n",
      "1      2  Thank you @TheOnion for dragging all of us und...      1\n",
      "2      3  #avetmissdone is catching on faster than the #...      1\n",
      "3      4  They just said #Tonysnell was back from the fl...      2\n",
      "4      5  Forget locking them up on an island to die slo...      2\n"
     ]
    }
   ],
   "source": [
    "#import tweet data\n",
    "data = pd.read_csv(\"tweets_tagged.csv\",delimiter=',',encoding='latin-1')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6300 entries, 0 to 9999\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Sr No   6300 non-null   int64 \n",
      " 1   tweet   6300 non-null   object\n",
      " 2   label   6300 non-null   int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 196.9+ KB\n",
      "None\n",
      "    Sr No                                              tweet  label\n",
      "0       1  Hysteria surrounding #coronavirus NZ daycare r...      3\n",
      "1       2  Thank you @TheOnion for dragging all of us und...      1\n",
      "2       3  #avetmissdone is catching on faster than the #...      1\n",
      "5       6  Please keep posted!?????? #CoronaVirus #Gensan...      3\n",
      "6       7  What did Richard Jefferson say? ?? #coronaviru...      3\n",
      "8       9  Halt???\\nall??commercial??flights??from #China...      3\n",
      "10     11  #WuhanCoronavirus #Coronavirus\\nA young man wa...      1\n",
      "11     12  #WuhanCoronavirus #Coronavirus\\nA young man wa...      1\n",
      "13     14  Japan flies citizens home from virus-hit Wuhan...      1\n",
      "15     16  FUTURES triple digits down &amp; not too happy...      3\n"
     ]
    }
   ],
   "source": [
    "#positive, negative tweets\n",
    "pos_tweets =data['tweet'][data['label']==1]\n",
    "neg_tweets = data['tweet'][data['label']==3]\n",
    "pos_neg_tweets = data[data['label']!=2]\n",
    "\n",
    "#examine the data\n",
    "print(pos_neg_tweets.info())\n",
    "print(pos_neg_tweets[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hysteria', 'surrounding', '#', 'coronavirus', 'NZ', 'daycare', 'requesting', 'all', 'children', 'who']\n"
     ]
    }
   ],
   "source": [
    "#word tokenizer on all positive and negative tweets to count all words -NOT USED\n",
    "from nltk.tokenize import word_tokenize\n",
    "tweet_tokens = []\n",
    "for row in pos_neg_tweets['tweet']:\n",
    "    row_tokens = word_tokenize(row)\n",
    "    for word in row_tokens:\n",
    "        tweet_tokens.append(word)\n",
    "print(tweet_tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize positive and negative tweets\n",
    "pos_tokens = []\n",
    "for tweet in pos_tweets:\n",
    "    pos_tokens.append( word_tokenize(tweet))\n",
    "neg_tokens=[]\n",
    "for tweet in neg_tweets:\n",
    "    neg_tokens.append( word_tokenize(tweet)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hysteria', 'NNP'), ('surrounding', 'VBG'), ('#', '#'), ('coronavirus', 'NN'), ('NZ', 'NNP'), ('daycare', 'NN'), ('requesting', 'VBG'), ('all', 'DT'), ('children', 'NNS'), ('who', 'WP'), ('have', 'VBP'), ('visited', 'VBN'), ('a', 'DT'), ('country', 'NN'), ('with', 'IN'), ('any', 'DT'), ('confirmed', 'JJ'), ('cases', 'NNS'), ('be', 'VB'), ('excluded', 'VBN'), ('for', 'IN'), ('2', 'CD'), ('weeks', 'NNS'), ('.', '.'), ('This', 'DT'), ('includes', 'VBZ'), ('Australia', 'NNP'), ('.', '.'), ('So', 'RB'), (',', ','), ('despite', 'IN'), ('us', 'PRP'), ('only', 'RB'), ('visiting', 'VBG'), ('Adelaide', 'NNP'), ('where', 'WRB'), ('there', 'EX'), ('are', 'VBP'), ('no', 'DT'), ('confirmed', 'JJ'), ('cases', 'NNS'), (',', ','), ('we', 'PRP'), ('are', 'VBP'), ('in', 'IN'), ('this', 'DT'), ('category', 'NN'), ('?', '.'), ('?', '.')]\n"
     ]
    }
   ],
   "source": [
    "#normalization \n",
    "\n",
    "#tagging position of word in sentence\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "#show example for first tweet\n",
    "print(pos_tag(neg_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hysteria', 'surround', '#', 'coronavirus', 'NZ', 'daycare', 'request', 'all', 'child', 'who', 'have', 'visit', 'a', 'country', 'with', 'any', 'confirmed', 'case', 'be', 'exclude', 'for', '2', 'week', '.', 'This', 'include', 'Australia', '.', 'So', ',', 'despite', 'us', 'only', 'visit', 'Adelaide', 'where', 'there', 'be', 'no', 'confirmed', 'case', ',', 'we', 'be', 'in', 'this', 'category', '?', '?']\n"
     ]
    }
   ],
   "source": [
    "#apply lemmatizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def lemmatize_sentence(tokens): #copy and pasted from blog\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "print(lemmatize_sentence(neg_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove noise (copy and pasted from blog)\n",
    "import re, string\n",
    "\n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\freudenreich\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#download stopwords to remove\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hysteria', 'surround', 'coronavirus', 'nz', 'daycare', 'request', 'child', 'visit', 'country', 'confirmed', 'case', 'exclude', '2', 'week', 'include', 'australia', 'despite', 'us', 'visit', 'adelaide', 'confirmed', 'case', 'category']\n"
     ]
    }
   ],
   "source": [
    "#apply remove_noise function\n",
    "pos_tokens_cleaned = []\n",
    "neg_tokens_cleaned = []\n",
    "\n",
    "for tokens in pos_tokens:\n",
    "    pos_tokens_cleaned.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "for tokens in neg_tokens:\n",
    "    neg_tokens_cleaned.append(remove_noise(tokens, stop_words))\n",
    "    \n",
    "print(neg_tokens_cleaned[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coronavirus', 'http', 'china', \"'s\", 'virus', 'coronavirusoutbreak', 'case', 'wuhan', '...', 'people']\n"
     ]
    }
   ],
   "source": [
    "#count words in all pos and neg tweets to later drop most frequent words (after transforming to lower case)\n",
    "\n",
    "word_counter={}\n",
    "for data in pos_tokens_cleaned, neg_tokens_cleaned:\n",
    "    for tweet in data:\n",
    "        for word in tweet:\n",
    "            if word.lower() in word_counter:\n",
    "                word_counter[word.lower()] +=1\n",
    "            else:\n",
    "                word_counter[word.lower()] = 1\n",
    "\n",
    "popular_words = sorted(word_counter, key = word_counter.get, reverse = True)\n",
    "top_10=popular_words[:10]\n",
    "\n",
    "\n",
    "#print top 10 words\n",
    "print(top_10)  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all words in pos and neg tweets as list\n",
    "pos_all_words=[]\n",
    "neg_all_words=[]\n",
    "\n",
    "for tokens in neg_tokens_cleaned:\n",
    "    for token in tokens:\n",
    "        neg_all_words.append(token)\n",
    "        \n",
    "for tokens in pos_tokens_cleaned:\n",
    "    for token in tokens:\n",
    "        pos_all_words.append(token)    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop 50 most common words in positive and negative tweets\n",
    "pos_all_words_10 = list(set(pos_all_words).difference(top_10))\n",
    "neg_all_words_10 = list(set(neg_all_words).difference(top_10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coronavirus', 'http', 'china', \"'s\", 'coronavirusoutbreak', 'virus', '...', 'wuhan', 'people', 'amp']\n"
     ]
    }
   ],
   "source": [
    "#count words in positive tweets\n",
    "\n",
    "count_pos={}\n",
    "for word in pos_all_words:\n",
    "    if word.lower() in count_pos:\n",
    "        count_pos[word.lower()] +=1\n",
    "    else:\n",
    "        count_pos[word.lower()] = 1\n",
    "\n",
    "top_pos_words = sorted(count_pos, key = count_pos.get, reverse = True)\n",
    "print(top_pos_words[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternative with get_all_words\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "pos_all_words2 = get_all_words(pos_tokens_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('coronavirus', 3919), ('http', 2413), ('china', 709), (\"'s\", 362), ('coronavirusoutbreak', 348), ('virus', 336), ('...', 319), ('wuhan', 315), ('people', 307), ('amp', 267)]\n"
     ]
    }
   ],
   "source": [
    "#word frequencies for positive words\n",
    "from nltk import FreqDist\n",
    "\n",
    "freq_dist_pos = FreqDist(pos_all_words2)\n",
    "\n",
    "print(freq_dist_pos.most_common(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
